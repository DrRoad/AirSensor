---
title: "Writing Custom Aggregation Functions"
author: "Mazama Science"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Temporal Aggregation

An essential piece of analysis of large data is efficient granularization: computing aggregations like `sum`, `mean`, `sd`, `min`, and `max`, in which a single number gives insight into the nature of a potentially large dataset. _Time series_ aggregation is the aggregation of all data points over a specified _period_.  In the **AirSensor** context, this is achieved via `pat_aggregate()`, using a function similar to those mentioned above, over a (default) 1-hour period. The result of the aggregation is one data point that reflects a statistical view of the collected and aggregated data points over the hour.

To demonstrate this feature we'll load a 24-hour period of sensor and compare the data to the aggregated data.

```{r}
# AirSensor setup
library(AirSensor)
setArchiveBaseUrl("https://airfire-data-exports.s3-us-west-2.amazonaws.com/PurpleAir/v1")

# Load the PurpleAir sensor data
pas <- pas_load()
pat <- pat_load(label = 'SCSC_33', pas = pas) %>% pat_filterDate(20200501, 20200502)
```

A standard 24-hour period of non-aggregated data typically consists of 720 data entries, an entry every 2 minutes.
```{r}
nrow(pat$data)
```

Here is a multi-plot of how the non-aggregated data looks.

```{r}
p1 <- pat_multiplot(pat, sampleSize = NULL)
p1
```

#### 1-hour Aggregation

Using `pat_aggregate()` we can aggregate the `pat` object to an hourly average of the data.

```{r}
hourly_pat <- pat_aggregate(pat)
nrow(hourly_pat$data)
```
As we'd expect, an hourly aggregated `pat` contains 24 entries, an entry every hour.



```{r}
p2 <- pat_multiplot(hourly_pat)
p2
```

#### Advanced Aggregation Methods

In order to write custom aggregation functions for use with `pat_aggregate()`, we must first familiarize ourselves with `pat_aggregate()`'s underlying algorithm. When executed, `pat_aggregate(pat, FUN)` utilizes the `datetime` axis of a PurpleAir time series object (`pat`) to split the data into time-granular bins. Each bin has the supplied function `FUN` mapped to the numeric vectors within the `pat` data. `FUN` is capable of accepting _any_ valid R function, and as with any great power comes great responsibility. The only requirements for the function are `FUN` _must_ operate on univariate numeric data and return a scalar value (think`sum`, or `mean`). The last step in `pat_aggregate()` is to combine the transformed bins along a shortened `datetime` axis and return a data object of the same `pat` class (`pa_timeseries`).

With care, we can extend the use of `pat_aggregate()` to summarize time series `pat` data of nearly _any_ period.
You can create different aggregation periods by explicitly providing `f`, string describing the period to split by `f =` `'hours'`, `'minutes'`, `'weeks'`, `'months'`, etc. and `k`, a number of periods to aggregate in each bin. For example, a 15-minute standard deviation (`sd`) aggregation would look as follows:

```{r}
# Aggregate the standard deviation of 15-minute periods
sd_fifteen_minute_pat <- pat_aggregate(pat, function(x) { sd(x, na.rm = TRUE) }, unit = 'minutes', count = 15)

# View first 5 entries of data
head(sd_fifteen_minute_pat$data)
```

Aggregation by different periods is an important concept to recognize for advanced usage because it can be utilized in a number of ways, such as  writing robust quality-control algorithms or speeding up calculations.

